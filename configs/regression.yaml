MODEL_NAME: "meta-llama/Llama-3.2-3B-Instruct"
# MODEL_NAME = "lianghsun/Llama-3.2-Taiwan-3B-Instruct"

USE_FP16: True  
USE_FLASH_ATTENTION: False  

# LoRA settings for fine-tuning
LORA_R : 8
LORA_ALPHA : 16
LORA_DROPOUT : 0.1
TARGET_MODULES : ["q_proj", "v_proj"] 

# Training settings
EPOCHS : 5
BATCH_SIZE : 1
GRADIENT_ACCUMULATION_STEPS : 2
LEARNING_RATE : 1e-4
WEIGHT_DECAY : 0.01
MAX_LENGTH : 512

MODEL_TOKEN : "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"
# MODEL_TOKEN = "Put your own token"