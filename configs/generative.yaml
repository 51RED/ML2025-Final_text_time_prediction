MODEL_NAME: "meta-llama/Llama-3.2-3B-Instruct"
# MODEL_NAME = "lianghsun/Llama-3.2-Taiwan-3B-Instruct"

USE_FP16: True  
USE_FLASH_ATTENTION: False  

# LoRA settings for fine-tuning
LORA_R : 8
LORA_ALPHA : 16
LORA_DROPOUT : 0.1
TARGET_MODULES : ["q_proj", "v_proj"] 

# Training settings
EPOCHS : 5
BATCH_SIZE : 1
GRADIENT_ACCUMULATION_STEPS : 8
LEARNING_RATE : 1e-4
WEIGHT_DECAY : 0.01
MAX_LENGTH : 512
MAX_NEW_TOKENS : 20

# Few-shot settings
NUM_FEW_SHOT_EXAMPLES : 5

# Inference settings
TEMPERATURE : 0.1  # Low temperature for consistent predictions
DO_SAMPLE : False  # Use greedy decoding

HF_TOKEN : "Put your own token"